{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1511.06581\n",
    "http://coach.nervanasys.com/algorithms/value_optimization/dueling_dqn/index.html\n",
    "http://torch.ch/blog/2016/04/30/dueling_dqn.html\n",
    "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df\n",
    "https://github.com/dxyang/DQN_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "CAPACITY = 10_000\n",
    "BATCH_SIZE = 32\n",
    "GAME = 'PongNoFrameskip-v4'\n",
    "N_ACTIONS = gym.make(GAME).action_space.n\n",
    "LEARNING_START = CAPACITY\n",
    "UPDATE_FREQ = 1\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_STEPS = 30_000\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE = 1_000\n",
    "PRINT_UPDATE = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        \"\"\"\n",
    "        Replay memory that holds examples in the form of (s, a, r, s')\n",
    "        \n",
    "        args:\n",
    "            capacity (int): the size of the memory\n",
    "            batch_size (int): size of batches used for training model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        #the memory holds al the (s, a, r, s') pairs\n",
    "        #a deque is first-in-first-out, i.e. when you push an example onto the queue\n",
    "        #and it at maximum capacity, the oldest example is popped off the queue\n",
    "        self.memory = deque(maxlen=self.capacity) \n",
    "        \n",
    "        #examples in the queue are saved as Transitions\n",
    "        #makes the code more readable and intuitive when getting  examples from the memory\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state')) \n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Places an (s, a, r, s') example in the memory\n",
    "        \n",
    "        args:\n",
    "            state (np.array): the observation obtained from the environment before the action\n",
    "            action (list[int]): the action taken\n",
    "            reward (list[int]): the reward received from the action taken at the current state\n",
    "            next_state (np.array or None): the observation obtained from the environment after the action,\n",
    "                                           is None when the state is a terminal state\n",
    "        \"\"\"\n",
    "        \n",
    "        #convert all to tensors\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.LongTensor([action])\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        if next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "        \n",
    "        #create a transition\n",
    "        transition = self.Transition(state=state, action=action, reward=reward, next_state=next_state)\n",
    "        \n",
    "        #add to the memory\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Gets a random sample of n = batch_size examples from the memory\n",
    "        The transition returned contains n of each elements, i.e. a batch_size of 32\n",
    "        means this will return a tuple of (32 states, 32 actions, 32 rewards, 32 next_states)\n",
    "            \n",
    "        returns:\n",
    "            Transitions (namedtuple): a tuple of (s, a, r, s'), \n",
    "        \"\"\"\n",
    "        \n",
    "        #sample batch_size transitions for the memory\n",
    "        transitions = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        #unzip and then rezip so each element contains batch_size examples \n",
    "        return self.Transition(*(zip(*transitions)))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the memory, i.e. number of examples in the memory\n",
    "        \n",
    "        returns:\n",
    "            length (int): number of examples in the memory\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) \n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "    \n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs): \n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "    \n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "    \n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]    \n",
    "    \n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Image shape to num_channels x weight x height\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.swapaxes(observation, 2, 0)\n",
    "    \n",
    "def wrap_pytorch(env):\n",
    "    return ImageToPyTorch(env)\n",
    "    \n",
    "def make_atari(env_id):\n",
    "    env = gym.make(env_id)\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    return env\n",
    "\n",
    "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.a_fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.a_fc2 = nn.Linear(512, n_actions) #actions from from env.action_space.n\n",
    "\n",
    "        self.v_fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.v_fc2 = nn.Linear(512, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1) #flattens the (N, C, H, W) to (N, C*H*W)\n",
    "        \n",
    "        a = F.relu(self.a_fc1(x))\n",
    "        a = self.a_fc2(a)\n",
    "        \n",
    "        v = F.relu(self.v_fc1(x))\n",
    "        v = self.v_fc2(v)\n",
    "        \n",
    "        return v + a - a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, mem, model, update_freq, learning_start, e_start, e_end, e_steps, gamma, target_update, print_update):\n",
    "        \"\"\"\n",
    "        An agent class that handles training the model\n",
    "\n",
    "        args:\n",
    "            mem (ReplayMemory): ReplayMemory object\n",
    "            env (Environment): Environment object\n",
    "            model (nn.Module): PyTorch model\n",
    "            update_freq (int): we only update the model every update_freq steps, 1 means update every step\n",
    "            learning_start (int): we only start updating the model after learning_start steps\n",
    "            e_start (int): initial value of epsilon\n",
    "            e_end (int): minimum value of epsilon\n",
    "            e_steps (int): controls the rate of decay from e_start to e_end\n",
    "            gamma (float): decay rate of rewards\n",
    "            target_update (int): update target model after this many parameter updates\n",
    "            print_update (int): print summary of performance after this many steps\n",
    "        \"\"\"\n",
    "        \n",
    "        self.env = env\n",
    "        self.mem = mem\n",
    "        self.model = model\n",
    "        self.update_freq = update_freq\n",
    "        self.learning_start = learning_start\n",
    "        self.e_start = e_start\n",
    "        self.e_end = e_end\n",
    "        self.e_steps = e_steps\n",
    "        self.gamma = gamma\n",
    "        self.target_update = target_update\n",
    "        self.print_update = print_update\n",
    "        \n",
    "        self.steps = 0 #number of steps taken\n",
    "        self.episodes = 0 #number of episodes\n",
    "        \n",
    "        #put model on gpu if available\n",
    "        self.model = model.to(device)\n",
    "        \n",
    "        #create target model\n",
    "        #set to evaluation mode to turn off batch-norm/dropout if used\n",
    "        self.target = copy.deepcopy(self.model)\n",
    "        self.target.eval()\n",
    "    \n",
    "        #create optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        \n",
    "    def get_epsilon(self):\n",
    "        \"\"\"\n",
    "        Calculates the value of epsilon from the current number of steps\n",
    "        \n",
    "        returns:\n",
    "            epsilon (float): the probability of doing a random action\n",
    "        \"\"\"\n",
    "        epsilon = self.e_end + (self.e_start - self.e_end) * math.exp(-1. * self.steps / self.e_steps)\n",
    "        return epsilon\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects action to perform, with probability = epsilon chooses a random action,\n",
    "        else chooses the best predicted action of the model\n",
    "        \n",
    "        args:\n",
    "            state (np.array): input state to the model\n",
    "            \n",
    "        returns:\n",
    "            action (int): the index of the action \n",
    "        \"\"\"\n",
    "    \n",
    "        #get value of epsilon\n",
    "        epsilon = self.get_epsilon()\n",
    "        \n",
    "        #with probablity of epsilon, pick a random action\n",
    "        if random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            #with probability of (1 - epsilon) pick predicted value\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(device) #convert to tensor, reshape and add to gpu\n",
    "                Qsa = self.model(state) #pass state through model to get Qsa\n",
    "                action = Qsa.max(1)[1].item() #action is max Qsa value\n",
    "                \n",
    "        #make sure the value is an integer\n",
    "        assert isinstance(action, int)\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop of the model\n",
    "        \n",
    "        Algorithm:\n",
    "        \n",
    "        While true:\n",
    "            - resets environment and gets initial state\n",
    "            While episode is not done:\n",
    "                - selects which action to take\n",
    "                - performs action on environment and receives next state, reward and if the episode has ended\n",
    "                - sums rewards across episode\n",
    "                - pushes the (s, a, r, s') tuple onto the memory\n",
    "                - updates the current state to be the state receives from the environment\n",
    "                - increase total number of steps and steps within episode\n",
    "                - updates model parameters every update_freq steps and only after learning_start steps\n",
    "                - updates target model after target_update parameter updates and only after learning_start steps\n",
    "                - prints summary every print_update steps\n",
    "            - increase number of episodes\n",
    "            - update list of all total episode rewards\n",
    "        \"\"\"\n",
    "        \n",
    "        training_done = False\n",
    "        reward_per_episode = []\n",
    "        \n",
    "        while not training_done:\n",
    "            \n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            \n",
    "            #get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            while not episode_done:\n",
    "                                \n",
    "                #get action\n",
    "                action = self.get_action(state)\n",
    "                               \n",
    "                #apply action while skipping frames\n",
    "                next_state, reward, episode_done, info = self.env.step(action)\n",
    "\n",
    "                #sum rewards\n",
    "                episode_reward += reward\n",
    "       \n",
    "                #add to memory, if episode has finished set next_state to None\n",
    "                mem.push(state, action, reward, None if episode_done else next_state)\n",
    "                    \n",
    "                #make next_state the new state\n",
    "                state = next_state\n",
    "                                \n",
    "                #increase number of steps\n",
    "                self.steps += 1\n",
    "                episode_steps += 1 \n",
    "            \n",
    "                #update model parameters\n",
    "                if self.steps % self.update_freq == 0 and self.steps > self.learning_start:\n",
    "                    loss = self.optimize()\n",
    "            \n",
    "                #update target model\n",
    "                if self.steps % (self.target_update*self.update_freq) == 0 and self.steps > self.learning_start:\n",
    "                    self.target.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "                #print summary\n",
    "                if self.steps % self.print_update == 0:\n",
    "                    avg_reward_per_episode = np.mean(reward_per_episode[-10:]) #average reward of last 10 episodes\n",
    "                    reward_per_episode = []\n",
    "                    print(f'Episodes: {self.episodes}, Steps: {self.steps}, Epsilon: {self.get_epsilon():.2f}, Avg. Reward per Ep: {avg_reward_per_episode:.2f}')\n",
    "\n",
    "            #increase number of episodes\n",
    "            self.episodes += 1\n",
    "            reward_per_episode.append(episode_reward)\n",
    "                            \n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Update model parameters\n",
    "        \n",
    "        Algorithm:\n",
    "        \n",
    "        - get a batch of transitions\n",
    "        - find out which next_states are terminal states\n",
    "        - \n",
    "        \"\"\"\n",
    "        \n",
    "        #get a batch\n",
    "        transitions = mem.sample()\n",
    "        \n",
    "        #need to set the Q value of terminal states to 0\n",
    "        #this mask will be 1 for non-terminal next_states and 0 for terminal next_states\n",
    "        non_terminal_mask = torch.ByteTensor(list(map(lambda ns: ns is not None, transitions.next_state)))\n",
    "        \n",
    "        #this will be 1 for terminal next_states, and 0 for non-terminal next states\n",
    "        terminal_mask = 1 - non_terminal_mask\n",
    "                \n",
    "        #state_batch = (N*C,H,W), where N is batch_size, C is phi_length, H and W state height and width\n",
    "        state_batch = torch.cat(transitions.state).to(device)\n",
    "        \n",
    "        #action_batch = (N, 1)\n",
    "        action_batch = torch.cat(transitions.action).unsqueeze(1).to(device)\n",
    "        \n",
    "        #reward_batch = (N, 1)\n",
    "        reward_batch = torch.cat(transitions.reward).unsqueeze(1).to(device)\n",
    "        \n",
    "        #next_state_batch = (M*C,H,W), where M is number of non_terminal next_state in the batch\n",
    "        non_terminal_next_state_batch = torch.cat([ns for ns in transitions.next_state if ns is not None]).to(device)\n",
    "        \n",
    "        #reshape to (N,C,H,W)\n",
    "        state_batch = state_batch.view(mem.batch_size, 4, 84, 84)\n",
    "        \n",
    "        #reshape to (V,C,H,W)\n",
    "        non_terminal_next_state_batch = non_terminal_next_state_batch.view(-1, 4, 84, 84)\n",
    "        \n",
    "        #get predicted Q values from model\n",
    "        Q_preds = self.model(state_batch)\n",
    "        \n",
    "        #get Q values of action taken, shape (N,1)\n",
    "        Q_vals = Q_preds.gather(1, action_batch)\n",
    "          \n",
    "        #get Q values from target model  \n",
    "        target_pred = self.target(non_terminal_next_state_batch)\n",
    "\n",
    "        #tensor for placing target values\n",
    "        target_vals = torch.zeros(mem.batch_size, 1).to(device) \n",
    "\n",
    "        #fill in target values for non_terminal states\n",
    "        #the terminal states will stay initialized as zeros\n",
    "        target_vals[non_terminal_mask] = target_pred.max(1)[0].unsqueeze(1)\n",
    "            \n",
    "        expected_vals = reward_batch + (target_vals * self.gamma)\n",
    "            \n",
    "        #calculate loss between Q values and target values\n",
    "        loss = F.smooth_l1_loss(Q_vals, expected_vals.detach())\n",
    "            \n",
    "        #zero gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        #calculate gradients \n",
    "        loss.backward()\n",
    "        \n",
    "        #clamp gradients\n",
    "        for p in self.model.parameters():\n",
    "            p.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        #update parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari(GAME)\n",
    "env = wrap_deepmind(env, frame_stack=True)\n",
    "env = wrap_pytorch(env)\n",
    "env.seed(SEED)\n",
    "\n",
    "mem = ReplayMemory(CAPACITY, BATCH_SIZE)\n",
    "model = DQN(N_ACTIONS)\n",
    "agent = Agent(env, mem, model, UPDATE_FREQ, LEARNING_START, EPSILON_START, EPSILON_END, EPSILON_STEPS, GAMMA, TARGET_UPDATE, PRINT_UPDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 5, Steps: 5000, Epsilon: 0.85, Avg. Reward per Ep: -20.40\n",
      "Episodes: 10, Steps: 10000, Epsilon: 0.72, Avg. Reward per Ep: -20.00\n",
      "Episodes: 15, Steps: 15000, Epsilon: 0.61, Avg. Reward per Ep: -19.80\n",
      "Episodes: 20, Steps: 20000, Epsilon: 0.52, Avg. Reward per Ep: -20.20\n",
      "Episodes: 26, Steps: 25000, Epsilon: 0.44, Avg. Reward per Ep: -20.17\n",
      "Episodes: 30, Steps: 30000, Epsilon: 0.37, Avg. Reward per Ep: -19.50\n",
      "Episodes: 34, Steps: 35000, Epsilon: 0.32, Avg. Reward per Ep: -18.50\n",
      "Episodes: 38, Steps: 40000, Epsilon: 0.27, Avg. Reward per Ep: -15.25\n",
      "Episodes: 41, Steps: 45000, Epsilon: 0.23, Avg. Reward per Ep: -17.67\n",
      "Episodes: 44, Steps: 50000, Epsilon: 0.20, Avg. Reward per Ep: -16.33\n",
      "Episodes: 47, Steps: 55000, Epsilon: 0.17, Avg. Reward per Ep: -18.00\n",
      "Episodes: 50, Steps: 60000, Epsilon: 0.14, Avg. Reward per Ep: -16.67\n",
      "Episodes: 52, Steps: 65000, Epsilon: 0.12, Avg. Reward per Ep: -17.00\n",
      "Episodes: 55, Steps: 70000, Epsilon: 0.11, Avg. Reward per Ep: -15.00\n",
      "Episodes: 57, Steps: 75000, Epsilon: 0.09, Avg. Reward per Ep: -16.50\n",
      "Episodes: 59, Steps: 80000, Epsilon: 0.08, Avg. Reward per Ep: -15.50\n",
      "Episodes: 61, Steps: 85000, Epsilon: 0.07, Avg. Reward per Ep: -14.00\n",
      "Episodes: 64, Steps: 90000, Epsilon: 0.06, Avg. Reward per Ep: -13.00\n",
      "Episodes: 65, Steps: 95000, Epsilon: 0.05, Avg. Reward per Ep: -9.00\n",
      "Episodes: 67, Steps: 100000, Epsilon: 0.05, Avg. Reward per Ep: -13.50\n",
      "Episodes: 69, Steps: 105000, Epsilon: 0.04, Avg. Reward per Ep: -5.00\n",
      "Episodes: 71, Steps: 110000, Epsilon: 0.04, Avg. Reward per Ep: -9.50\n",
      "Episodes: 72, Steps: 115000, Epsilon: 0.03, Avg. Reward per Ep: -6.00\n",
      "Episodes: 74, Steps: 120000, Epsilon: 0.03, Avg. Reward per Ep: -12.50\n",
      "Episodes: 76, Steps: 125000, Epsilon: 0.03, Avg. Reward per Ep: -10.00\n",
      "Episodes: 78, Steps: 130000, Epsilon: 0.02, Avg. Reward per Ep: -7.00\n",
      "Episodes: 80, Steps: 135000, Epsilon: 0.02, Avg. Reward per Ep: -9.50\n",
      "Episodes: 82, Steps: 140000, Epsilon: 0.02, Avg. Reward per Ep: -11.00\n",
      "Episodes: 84, Steps: 145000, Epsilon: 0.02, Avg. Reward per Ep: -10.00\n",
      "Episodes: 85, Steps: 150000, Epsilon: 0.02, Avg. Reward per Ep: -7.00\n",
      "Episodes: 87, Steps: 155000, Epsilon: 0.02, Avg. Reward per Ep: -9.00\n",
      "Episodes: 88, Steps: 160000, Epsilon: 0.01, Avg. Reward per Ep: -1.00\n",
      "Episodes: 90, Steps: 165000, Epsilon: 0.01, Avg. Reward per Ep: 1.50\n",
      "Episodes: 91, Steps: 170000, Epsilon: 0.01, Avg. Reward per Ep: 5.00\n",
      "Episodes: 93, Steps: 175000, Epsilon: 0.01, Avg. Reward per Ep: -6.00\n",
      "Episodes: 94, Steps: 180000, Epsilon: 0.01, Avg. Reward per Ep: 2.00\n",
      "Episodes: 96, Steps: 185000, Epsilon: 0.01, Avg. Reward per Ep: -2.00\n",
      "Episodes: 98, Steps: 190000, Epsilon: 0.01, Avg. Reward per Ep: 11.00\n",
      "Episodes: 99, Steps: 195000, Epsilon: 0.01, Avg. Reward per Ep: -6.00\n",
      "Episodes: 101, Steps: 200000, Epsilon: 0.01, Avg. Reward per Ep: 5.00\n",
      "Episodes: 103, Steps: 205000, Epsilon: 0.01, Avg. Reward per Ep: 3.50\n",
      "Episodes: 106, Steps: 210000, Epsilon: 0.01, Avg. Reward per Ep: 10.33\n",
      "Episodes: 108, Steps: 215000, Epsilon: 0.01, Avg. Reward per Ep: 16.00\n",
      "Episodes: 110, Steps: 220000, Epsilon: 0.01, Avg. Reward per Ep: 5.00\n",
      "Episodes: 113, Steps: 225000, Epsilon: 0.01, Avg. Reward per Ep: 19.33\n",
      "Episodes: 116, Steps: 230000, Epsilon: 0.01, Avg. Reward per Ep: 16.67\n",
      "Episodes: 118, Steps: 235000, Epsilon: 0.01, Avg. Reward per Ep: 9.00\n",
      "Episodes: 120, Steps: 240000, Epsilon: 0.01, Avg. Reward per Ep: 9.00\n",
      "Episodes: 123, Steps: 245000, Epsilon: 0.01, Avg. Reward per Ep: 20.00\n",
      "Episodes: 125, Steps: 250000, Epsilon: 0.01, Avg. Reward per Ep: 8.50\n",
      "Episodes: 127, Steps: 255000, Epsilon: 0.01, Avg. Reward per Ep: 5.50\n",
      "Episodes: 129, Steps: 260000, Epsilon: 0.01, Avg. Reward per Ep: 7.00\n",
      "Episodes: 131, Steps: 265000, Epsilon: 0.01, Avg. Reward per Ep: 15.50\n",
      "Episodes: 134, Steps: 270000, Epsilon: 0.01, Avg. Reward per Ep: 18.00\n",
      "Episodes: 136, Steps: 275000, Epsilon: 0.01, Avg. Reward per Ep: 16.00\n",
      "Episodes: 139, Steps: 280000, Epsilon: 0.01, Avg. Reward per Ep: 15.33\n",
      "Episodes: 141, Steps: 285000, Epsilon: 0.01, Avg. Reward per Ep: 15.00\n",
      "Episodes: 144, Steps: 290000, Epsilon: 0.01, Avg. Reward per Ep: 19.00\n",
      "Episodes: 146, Steps: 295000, Epsilon: 0.01, Avg. Reward per Ep: 19.00\n",
      "Episodes: 149, Steps: 300000, Epsilon: 0.01, Avg. Reward per Ep: 17.67\n",
      "Episodes: 152, Steps: 305000, Epsilon: 0.01, Avg. Reward per Ep: 20.33\n",
      "Episodes: 154, Steps: 310000, Epsilon: 0.01, Avg. Reward per Ep: 16.50\n",
      "Episodes: 157, Steps: 315000, Epsilon: 0.01, Avg. Reward per Ep: 17.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3d92c9b1ff97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-5c27cdc31593>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;31m#update model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;31m#update target model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5c27cdc31593>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m#calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m#clamp gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch04/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch04/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
