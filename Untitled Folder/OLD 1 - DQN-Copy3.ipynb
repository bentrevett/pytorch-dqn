{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "- https://ai.intel.com/demystifying-deep-reinforcement-learning/\n",
    "- https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
    "\n",
    "- https://github.com/AndersonJo/dqn-pytorch/blob/master/dqn.py\n",
    "- https://github.com/hengyuan-hu/rainbow\n",
    "- https://github.com/transedward/pytorch-dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "CAPACITY = 10_000\n",
    "BATCH_SIZE = 32\n",
    "GAME = 'PongNoFrameskip-v4'\n",
    "N_ACTIONS = gym.make(GAME).action_space.n\n",
    "PHI_LENGTH = 4\n",
    "UPDATE_FREQ = 1\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_STEPS = 30_000\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE = 1_000\n",
    "PRINT_UPDATE = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        \"\"\"\n",
    "        Replay memory that holds examples in the form of (s, a, r, s')\n",
    "        \n",
    "        args:\n",
    "            capacity (int): the size of the memory\n",
    "            batch_size (int): size of batches used for training model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
    "        self._available = False\n",
    "\n",
    "    def put(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Places an (s, a, r, s') example in the memory\n",
    "        \n",
    "        args:\n",
    "            state (np.array): \n",
    "            action (list[int]):\n",
    "            reward (list[int]):\n",
    "            next_state (np.array or None):\n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.LongTensor([action])\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        if next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "        transition = self.Transition(state=state, action=action, reward=reward, next_state=next_state)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Gets a random sample of n = batch_size examples from the memory\n",
    "            \n",
    "        returns:\n",
    "            Transitions (namedtuple): a tuple of (s, a, r, s')\n",
    "        \"\"\"\n",
    "        \n",
    "        transitions = random.sample(self.memory, self.batch_size)\n",
    "        return self.Transition(*(zip(*transitions)))\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the memory\n",
    "        \n",
    "        returns:\n",
    "            length (int): number of examples in the memory\n",
    "        \"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "    def is_available(self):\n",
    "        \"\"\"\n",
    "        Returns True if we have enough examples within the memory\n",
    "        \n",
    "        returns:\n",
    "            available (bool): True if we have at least n = batch_size examples in the memory\n",
    "        \"\"\"\n",
    "        if self._available:\n",
    "            return True\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self._available = True\n",
    "        return self._available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) \n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "    \n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs): \n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "    \n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "    \n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]    \n",
    "    \n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Image shape to num_channels x weight x height\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.swapaxes(observation, 2, 0)\n",
    "    \n",
    "def wrap_pytorch(env):\n",
    "    return ImageToPyTorch(env)\n",
    "    \n",
    "def make_atari(env_id):\n",
    "    env = gym.make(env_id)\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    return env\n",
    "\n",
    "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions) #actions from from env.action_space.n\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1))) #flattens the (N, C, H, W) to (N, C*H*W)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, mem, model, phi_length, update_freq, e_start, e_end, e_steps, gamma, target_update, print_update):\n",
    "        \"\"\"\n",
    "        An agent class that handles training the model\n",
    "\n",
    "        args:\n",
    "            mem (ReplayMemory): ReplayMemory object\n",
    "            env (Environment): Environment object\n",
    "            model (nn.Module): PyTorch model\n",
    "            phi_length (int): number of observations to stack to make a state\n",
    "            frame_skip (int): we only use every n = frame_skip observations to make a state\n",
    "            e_start (int): initial value of epsilon\n",
    "            e_end (int): minimum value of epsilon\n",
    "            e_steps (int): number of steps for epsilon to go from e_start to e_end\n",
    "            gamma (float): decay rate of rewards\n",
    "            target_update (int): after how many steps (frames) to update target model\n",
    "            print_update (int): after how many steps (frames) to print summary of performance\n",
    "            \n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.mem = mem\n",
    "        self.model = model\n",
    "        self.phi_length = phi_length\n",
    "        self.update_freq = update_freq\n",
    "        self.e_start = e_start\n",
    "        self.e_end = e_end\n",
    "        self.e_steps = e_steps\n",
    "        self.gamma = gamma\n",
    "        self.target_update = target_update\n",
    "        self.print_update = print_update\n",
    "        \n",
    "        self.steps = 0 #number of steps taken\n",
    "        self.episodes = 0 #number of episodes\n",
    "        self.obs_buffer = deque(maxlen=phi_length) #for holding observations to be turned into states\n",
    "        \n",
    "        #put model on gpu if available\n",
    "        self.model = model.to(device)\n",
    "        \n",
    "        #create target model\n",
    "        self.target = copy.deepcopy(self.model)\n",
    "        self.target.eval()\n",
    "    \n",
    "        #create optimizer\n",
    "        #self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.00025, alpha=0.95, momentum=0.95)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        \n",
    "    def get_epsilon(self):\n",
    "        \"\"\"\n",
    "        Calculates the value of epsilon from the current number of frames\n",
    "        \n",
    "        returns:\n",
    "            epsilon (int): the probability of doing a random action\n",
    "        \"\"\"\n",
    "        epsilon = self.e_end + (self.e_start - self.e_end) * math.exp(-1. * self.steps / self.e_steps)\n",
    "        return epsilon\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects action to perform, with probability = epsilon chooses a random action,\n",
    "        else chooses the best predicted action of the model\n",
    "        \n",
    "        args:\n",
    "            state (np.array): input state to the model\n",
    "            \n",
    "        returns:\n",
    "            action (int): the index of the action \n",
    "        \"\"\"\n",
    "    \n",
    "        #get value of epsilon\n",
    "        epsilon = self.get_epsilon()\n",
    "        \n",
    "        #with probablity of epsilon, pick a random action\n",
    "        if random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            #with probability of (1 - epsilon) pick predicted value\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(device) #convert to tensor, reshape and add to gpu\n",
    "                Qsa = self.model(state) #pass state through model to get Qa\n",
    "                action = Qsa.max(1)[1].item() #action is max Qa value\n",
    "                \n",
    "        #make sure the value is an integer\n",
    "        assert isinstance(action, int)\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        training_done = False\n",
    "        reward_per_episode = []\n",
    "        rewards_all_episodes = []\n",
    "        \n",
    "        while not training_done:\n",
    "            \n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            \n",
    "            #get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            while not episode_done:\n",
    "                                \n",
    "                #get action\n",
    "                action = self.get_action(state)\n",
    "                               \n",
    "                #apply action while skipping frames\n",
    "                next_state, reward, episode_done, info = self.env.step(action)\n",
    "\n",
    "                #sum rewards\n",
    "                episode_reward += reward\n",
    "       \n",
    "                #add to memory, for terminal states, set next_state to None\n",
    "                if episode_done:\n",
    "                    mem.put(state, action, reward, None)\n",
    "                else:\n",
    "                    mem.put(state, action, reward, next_state)\n",
    "                    \n",
    "                #make new state the old next_state\n",
    "                state = next_state\n",
    "                                \n",
    "                #increase number of steps\n",
    "                self.steps += 1\n",
    "                episode_steps += 1 \n",
    "            \n",
    "                #update model parameters\n",
    "                if mem.is_available() and self.steps % self.update_freq == 0 and self.steps > 10_000:\n",
    "                    loss = self.optimize()\n",
    "            \n",
    "                if self.steps % (self.target_update*self.update_freq) == 0 and self.steps > 10_000:\n",
    "                    self.target.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "                if self.steps % self.print_update == 0:\n",
    "                    avg_reward_per_episode = np.mean(reward_per_episode)\n",
    "                    #rewards_all_episodes.extend(reward_per_episode)\n",
    "                    reward_per_episode = []\n",
    "                    print(f'Episodes: {self.episodes}, Steps: {self.steps}, Epsilon: {self.get_epsilon():.2f}, Avg. Reward per Ep: {avg_reward_per_episode:.2f}')\n",
    "\n",
    "            #increase number of episodes\n",
    "            self.episodes += 1\n",
    "            reward_per_episode.append(episode_reward)\n",
    "                            \n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Update model parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        #get a batch\n",
    "        transitions = mem.sample()\n",
    "        \n",
    "        #need to set the Q value of terminal states to 0\n",
    "        #this mask will be 1 for non-terminal next_states and 0 for terminal next_states\n",
    "        non_terminal_mask = torch.ByteTensor(list(map(lambda ns: ns is not None, transitions.next_state)))\n",
    "        \n",
    "        #this will be 1 for terminal next_states, and 0 for non-terminal next states\n",
    "        terminal_mask = 1 - non_terminal_mask\n",
    "        \n",
    "        #state_batch = (N*C,H,W), where N is batch_size, C is phi_length, H and W are processed obs size\n",
    "        state_batch = torch.cat(transitions.state).to(device)\n",
    "        \n",
    "        #action_batch = (N, 1)\n",
    "        action_batch = torch.cat(transitions.action).unsqueeze(1).to(device)\n",
    "        \n",
    "        #reward_batch = (N, 1)\n",
    "        reward_batch = torch.cat(transitions.reward).unsqueeze(1).to(device)\n",
    "        \n",
    "        #next_state_batch = (V*C,H,W), where V is non_terminal next_state\n",
    "        non_terminal_next_state_batch = torch.cat([ns for ns in transitions.next_state if ns is not None]).to(device)\n",
    "        \n",
    "        #reshape to (N,C,H,W)\n",
    "        state_batch = state_batch.view(mem.batch_size, self.phi_length, 84, 84)\n",
    "        \n",
    "        #reshape to (V,C,H,W)\n",
    "        non_terminal_next_state_batch = non_terminal_next_state_batch.view(-1, self.phi_length, 84, 84)\n",
    "        \n",
    "        #get predicted Q values from model\n",
    "        Q_pred = self.model(state_batch)\n",
    "        \n",
    "        #get Q values of action taken, shape (N,1)\n",
    "        Q_vals = Q_pred.gather(1, action_batch)\n",
    "          \n",
    "        with torch.no_grad():\n",
    "            #get Q values from target model  \n",
    "            target_pred = self.target(non_terminal_next_state_batch)\n",
    "\n",
    "            #tensor for placing target values\n",
    "            target_vals = torch.zeros(mem.batch_size, 1).to(device) \n",
    "\n",
    "            #fill in target values for non_terminal states\n",
    "            #the terminal states will stay initialized as zeros\n",
    "            target_vals[non_terminal_mask] = reward_batch[non_terminal_mask] + target_pred.max(1)[0].unsqueeze(1) * self.gamma\n",
    "            target_vals[terminal_mask] = reward_batch[terminal_mask]\n",
    "            \n",
    "        #calculate loss between Q values and target values\n",
    "        loss = F.smooth_l1_loss(Q_vals, target_vals.detach())\n",
    "            \n",
    "        #zero gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        #calculate gradients \n",
    "        loss.backward()\n",
    "        \n",
    "        #clamp gradients\n",
    "        for p in self.model.parameters():\n",
    "            p.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        #update parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari(GAME)\n",
    "env = wrap_deepmind(env, frame_stack=True)\n",
    "env = wrap_pytorch(env)\n",
    "env.seed(SEED)\n",
    "\n",
    "mem = ReplayMemory(CAPACITY, BATCH_SIZE)\n",
    "model = DQN(N_ACTIONS)\n",
    "agent = Agent(env, mem, model, PHI_LENGTH, UPDATE_FREQ, EPSILON_START, EPSILON_END, EPSILON_STEPS, GAMMA, TARGET_UPDATE, PRINT_UPDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 5, Steps: 5000, Epsilon: 0.85, Avg. Reward per Ep: -20.40\n",
      "Episodes: 10, Steps: 10000, Epsilon: 0.72, Avg. Reward per Ep: -20.00\n",
      "Episodes: 15, Steps: 15000, Epsilon: 0.61, Avg. Reward per Ep: -19.80\n",
      "Episodes: 21, Steps: 20000, Epsilon: 0.52, Avg. Reward per Ep: -20.83\n",
      "Episodes: 27, Steps: 25000, Epsilon: 0.44, Avg. Reward per Ep: -20.67\n",
      "Episodes: 31, Steps: 30000, Epsilon: 0.37, Avg. Reward per Ep: -20.00\n",
      "Episodes: 36, Steps: 35000, Epsilon: 0.32, Avg. Reward per Ep: -19.40\n",
      "Episodes: 41, Steps: 40000, Epsilon: 0.27, Avg. Reward per Ep: -19.20\n",
      "Episodes: 45, Steps: 45000, Epsilon: 0.23, Avg. Reward per Ep: -18.25\n",
      "Episodes: 48, Steps: 50000, Epsilon: 0.20, Avg. Reward per Ep: -17.33\n",
      "Episodes: 52, Steps: 55000, Epsilon: 0.17, Avg. Reward per Ep: -18.50\n",
      "Episodes: 55, Steps: 60000, Epsilon: 0.14, Avg. Reward per Ep: -19.00\n",
      "Episodes: 58, Steps: 65000, Epsilon: 0.12, Avg. Reward per Ep: -16.67\n",
      "Episodes: 61, Steps: 70000, Epsilon: 0.11, Avg. Reward per Ep: -17.33\n",
      "Episodes: 64, Steps: 75000, Epsilon: 0.09, Avg. Reward per Ep: -14.33\n",
      "Episodes: 66, Steps: 80000, Epsilon: 0.08, Avg. Reward per Ep: -15.00\n",
      "Episodes: 69, Steps: 85000, Epsilon: 0.07, Avg. Reward per Ep: -14.33\n",
      "Episodes: 72, Steps: 90000, Epsilon: 0.06, Avg. Reward per Ep: -15.33\n",
      "Episodes: 75, Steps: 95000, Epsilon: 0.05, Avg. Reward per Ep: -17.33\n",
      "Episodes: 77, Steps: 100000, Epsilon: 0.05, Avg. Reward per Ep: -9.00\n",
      "Episodes: 79, Steps: 105000, Epsilon: 0.04, Avg. Reward per Ep: -15.50\n",
      "Episodes: 81, Steps: 110000, Epsilon: 0.04, Avg. Reward per Ep: -8.50\n",
      "Episodes: 83, Steps: 115000, Epsilon: 0.03, Avg. Reward per Ep: -13.00\n",
      "Episodes: 85, Steps: 120000, Epsilon: 0.03, Avg. Reward per Ep: -13.00\n",
      "Episodes: 87, Steps: 125000, Epsilon: 0.03, Avg. Reward per Ep: -10.50\n",
      "Episodes: 89, Steps: 130000, Epsilon: 0.02, Avg. Reward per Ep: -7.00\n",
      "Episodes: 91, Steps: 135000, Epsilon: 0.02, Avg. Reward per Ep: -6.50\n",
      "Episodes: 93, Steps: 140000, Epsilon: 0.02, Avg. Reward per Ep: -13.00\n",
      "Episodes: 94, Steps: 145000, Epsilon: 0.02, Avg. Reward per Ep: -6.00\n",
      "Episodes: 96, Steps: 150000, Epsilon: 0.02, Avg. Reward per Ep: -2.00\n",
      "Episodes: 98, Steps: 155000, Epsilon: 0.02, Avg. Reward per Ep: -5.00\n",
      "Episodes: 100, Steps: 160000, Epsilon: 0.01, Avg. Reward per Ep: -6.00\n"
     ]
    }
   ],
   "source": [
    "s, ns = agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(s[1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(s[2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(s[3,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
