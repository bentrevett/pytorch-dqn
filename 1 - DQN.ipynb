{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "- https://ai.intel.com/demystifying-deep-reinforcement-learning/\n",
    "- https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
    "\n",
    "- https://github.com/AndersonJo/dqn-pytorch/blob/master/dqn.py\n",
    "- https://github.com/hengyuan-hu/rainbow\n",
    "- https://github.com/transedward/pytorch-dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "CAPACITY = 1_000_000\n",
    "BATCH_SIZE = 32\n",
    "PROCESSED_SIZE = 84\n",
    "GAME = 'Pong-v0'\n",
    "N_ACTIONS = gym.make(GAME).action_space.n\n",
    "PHI_LENGTH = 4\n",
    "FRAME_SKIP = 4\n",
    "UPDATE_FREQ = 4\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_STEPS = 1_000_000\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE = 10_000\n",
    "PRINT_UPDATE = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        \"\"\"\n",
    "        Replay memory that holds examples in the form of (s, a, r, s')\n",
    "        \n",
    "        args:\n",
    "            capacity (int): the size of the memory\n",
    "            batch_size (int): size of batches used for training model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
    "        self._available = False\n",
    "\n",
    "    def put(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Places an (s, a, r, s') example in the memory\n",
    "        \n",
    "        args:\n",
    "            state (np.array): \n",
    "            action (list[int]):\n",
    "            reward (list[int]):\n",
    "            next_state (np.array or None):\n",
    "        \"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.LongTensor([action])\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        if next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "        transition = self.Transition(state=state, action=action, reward=reward, next_state=next_state)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Gets a random sample of n = batch_size examples from the memory\n",
    "            \n",
    "        returns:\n",
    "            Transitions (namedtuple): a tuple of (s, a, r, s')\n",
    "        \"\"\"\n",
    "        \n",
    "        transitions = random.sample(self.memory, self.batch_size)\n",
    "        return self.Transition(*(zip(*transitions)))\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the memory\n",
    "        \n",
    "        returns:\n",
    "            length (int): number of examples in the memory\n",
    "        \"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "    def is_available(self):\n",
    "        \"\"\"\n",
    "        Returns True if we have enough examples within the memory\n",
    "        \n",
    "        returns:\n",
    "            available (bool): True if we have at least n = batch_size examples in the memory\n",
    "        \"\"\"\n",
    "        if self._available:\n",
    "            return True\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self._available = True\n",
    "        return self._available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, game, size, seed):\n",
    "        \"\"\"\n",
    "        A class that has helpful wrappers around the Gym environment\n",
    "        \n",
    "        game (string): name of Atari game, i.e. Breakout-v0\n",
    "        size (int): height and width of observation after preprocessing\n",
    "        seed (int): random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size = size\n",
    "        \n",
    "        #init game\n",
    "        self.game = gym.make(game)\n",
    "        \n",
    "        #set random seed for determinism\n",
    "        self.game.seed(seed)\n",
    "        \n",
    "    def process(self, obs):\n",
    "        \"\"\"\n",
    "        Process an observation (i.e. convert to grayscale, resize and normalize)\n",
    "        \n",
    "        args:\n",
    "            obs (np.array): observation from gym of game screen, should be (height, width, channels)\n",
    "        \n",
    "        returns:\n",
    "            output (np.array): (self.size, self.size) array with all values <= 1\n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(obs.shape) == 3 #make sure image is correct shape\n",
    "        \n",
    "        gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY) #convert to grayscale\n",
    "        output = cv2.resize(gray, (self.size, self.size)) #resize\n",
    "        output = output.astype(np.float32, copy=False) #convert to float32\n",
    "        output /= 255.0 #normalize values between [0, 1]\n",
    "                     \n",
    "        assert (output <= 1.0).all() \n",
    "                        \n",
    "        return output\n",
    "    \n",
    "    def get_obs(self):\n",
    "        \"\"\"\n",
    "        Gets a processed observation\n",
    "        \n",
    "        returns:\n",
    "            obs (np.array): (self.size, self.size) array with all values <= 1\n",
    "        \"\"\"\n",
    "        \n",
    "        obs = self.game.render('rgb_array')\n",
    "        obs = self.process(obs)\n",
    "        return obs\n",
    "    \n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial state (unprocessed)\n",
    "        \n",
    "        returns:\n",
    "            obs (np.array): observation from gym of game screen, should be (height, width, channels)\n",
    "        \"\"\"\n",
    "        obs = self.game.reset()\n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial state (processed)\n",
    "        \n",
    "        returns:\n",
    "            output (np.array): (self.size, self.size) array with all values <= 1\n",
    "        \"\"\"\n",
    "        obs = self.game.reset()\n",
    "        output = self.process(obs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions) #actions from from env.action_space.n\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1))) #flattens the (N, C, H, W) to (N, C*H*W)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, mem, model, phi_length, frame_skip, e_start, e_end, e_steps, gamma, target_update, print_update):\n",
    "        \"\"\"\n",
    "        An agent class that handles training the model\n",
    "\n",
    "        args:\n",
    "            mem (ReplayMemory): ReplayMemory object\n",
    "            env (Environment): Environment object\n",
    "            model (nn.Module): PyTorch model\n",
    "            phi_length (int): number of observations to stack to make a state\n",
    "            frame_skip (int): we only use every n = frame_skip observations to make a state\n",
    "            e_start (int): initial value of epsilon\n",
    "            e_end (int): minimum value of epsilon\n",
    "            e_steps (int): number of steps for epsilon to go from e_start to e_end\n",
    "            gamma (float): decay rate of rewards\n",
    "            target_update (int): after how many steps (frames) to update target model\n",
    "            print_update (int): after how many steps (frames) to print summary of performance\n",
    "            \n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.mem = mem\n",
    "        self.model = model\n",
    "        self.phi_length = phi_length\n",
    "        self.frame_skip = frame_skip\n",
    "        self.e_start = e_start\n",
    "        self.e_end = e_end\n",
    "        self.e_steps = e_steps\n",
    "        self.gamma = gamma\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        self.steps = 0 #number of steps taken\n",
    "        self.episodes = 0 #number of episodes\n",
    "        self.obs_buffer = deque(maxlen=phi_length) #for holding observations to be turned into states\n",
    "        \n",
    "        #put model on gpu if available\n",
    "        self.model = model.to(device)\n",
    "        \n",
    "        #create target model\n",
    "        #TODO: this may need to be a copy.deepcopy or load state dict\n",
    "        self.target = copy.deepcopy(self.model)\n",
    "    \n",
    "        #create optimizer\n",
    "        #trying params from: https://github.com/hengyuan-hu/rainbow\n",
    "        #self.optimizer = optim.Adam(self.model.parameters(), lr=6.25e-5, eps=1.5e-4)\n",
    "        #from dqn paper\n",
    "        self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.00025, alpha=0.95, momentum=0.95)\n",
    "        \n",
    "    def get_epsilon(self):\n",
    "        \"\"\"\n",
    "        Calculates the value of epsilon from the current number of frames\n",
    "        \n",
    "        returns:\n",
    "            epsilon (int): the probability of doing a random action\n",
    "        \"\"\"\n",
    "        epsilon = self.e_end + (self.e_start - self.e_end) * math.exp(-1. * self.steps / self.e_steps)\n",
    "        return epsilon\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects action to perform, with probability = epsilon chooses a random action,\n",
    "        else chooses the best predicted action of the model\n",
    "        \n",
    "        args:\n",
    "            state (np.array): input state to the model\n",
    "            \n",
    "        returns:\n",
    "            action (int): the index of the action \n",
    "        \"\"\"\n",
    "    \n",
    "        #get value of epsilon\n",
    "        epsilon = self.get_epsilon()\n",
    "        \n",
    "        #with probablity of epsilon, pick a random action\n",
    "        if random.random() < epsilon:\n",
    "            action = self.env.game.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            #with probability of (1 - epsilon) pick predicted value\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(device) #convert to tensor, reshape and add to gpu\n",
    "                Qsa = self.model(state) #pass state through model to get Qa\n",
    "                action = Qsa.max(1)[1].item() #action is max Qa value\n",
    "                \n",
    "        #make sure the value is an integer\n",
    "        assert isinstance(action, int)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        \"\"\"\n",
    "        Get the initial state to the model, a stack of processed observations\n",
    "        \n",
    "        returns:\n",
    "            state (np.array): a stack of n = phi_length processed observations\n",
    "        \"\"\"\n",
    "        \n",
    "        _ = self.env.reset() #reset environment\n",
    "        obs = self.env.get_obs() #get a processed observation\n",
    "        state = np.stack([obs for _ in range(self.phi_length)], axis=0) #stack n = phi_length times to make a state\n",
    "        \n",
    "        #also fill the \n",
    "        for _ in range(self.phi_length):\n",
    "            self.obs_buffer.append(obs)\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get a stack from the observation buffer\n",
    "        \n",
    "        returns:\n",
    "            state (np.array): a stack of n = phi_length processed observations\n",
    "        \"\"\"\n",
    "        \n",
    "        state = np.array(self.obs_buffer)\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        training_done = False\n",
    "        reward_per_episode = []\n",
    "        rewards_all_episodes = []\n",
    "        \n",
    "        while not training_done:\n",
    "            \n",
    "            episode_done = False\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            \n",
    "            #get initial state\n",
    "            state = self.get_initial_state()\n",
    "            \n",
    "            while not episode_done:\n",
    "                                \n",
    "                #get action\n",
    "                action = self.get_action(state)\n",
    "                               \n",
    "                #apply action while skipping frames\n",
    "                observation, reward, episode_done, info = self.env.game.step(action)\n",
    "\n",
    "                #sum rewards\n",
    "                episode_reward += reward\n",
    "                    \n",
    "                #append processed observation to a buffer of observations\n",
    "                self.obs_buffer.append(self.env.get_obs())\n",
    "                        \n",
    "                #get the next state from the observation buffer\n",
    "                next_state = self.get_state()\n",
    "                \n",
    "                #add to memory, for terminal states, set next_state to None\n",
    "                if episode_done:\n",
    "                    mem.put(state, action, reward, None)\n",
    "                else:\n",
    "                    mem.put(state, action, reward, next_state)\n",
    "                    \n",
    "                #make new state the old next_state\n",
    "                state = next_state\n",
    "                                \n",
    "                #update model parameters\n",
    "                if mem.is_available() and self.steps % UPDATE_FREQ == 0:\n",
    "                    loss = self.optimize()\n",
    "    \n",
    "                #increase number of steps\n",
    "                self.steps += 1\n",
    "                episode_steps += 1 \n",
    "            \n",
    "                if self.steps % (TARGET_UPDATE*UPDATE_FREQ) == 0:\n",
    "                    self.target = copy.deepcopy(self.model)\n",
    "            \n",
    "                if self.steps % PRINT_UPDATE == 0:\n",
    "                    avg_reward_per_episode = np.mean(reward_per_episode)\n",
    "                    rewards_all_episodes.extend(reward_per_episode)\n",
    "                    reward_per_episode = []\n",
    "                    print(f'Episodes: {self.episodes}, Steps: {self.steps}, Epsilon: {self.get_epsilon():.2f}, Avg. Reward per Ep: {avg_reward_per_episode:.2f}')\n",
    "\n",
    "            #increase number of episodes\n",
    "            self.episodes += 1\n",
    "            reward_per_episode.append(episode_reward)\n",
    "                            \n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Update model parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        #get a batch\n",
    "        transitions = mem.sample()\n",
    "        \n",
    "        #need to set the Q value of terminal states to 0\n",
    "        #this mask will be 1 for non-terminal next_states and 0 for terminal next_states\n",
    "        non_terminal_mask = torch.ByteTensor(list(map(lambda ns: ns is not None, transitions.next_state)))\n",
    "        \n",
    "        #this will be 1 for terminal next_states, and 0 for non-terminal next states\n",
    "        terminal_mask = 1 - non_terminal_mask\n",
    "        \n",
    "        #state_batch = (N*C,H,W), where N is batch_size, C is phi_length, H and W are processed obs size\n",
    "        state_batch = torch.cat(transitions.state).to(device)\n",
    "        \n",
    "        #action_batch = (N, 1)\n",
    "        action_batch = torch.cat(transitions.action).unsqueeze(1).to(device)\n",
    "        \n",
    "        #reward_batch = (N, 1)\n",
    "        reward_batch = torch.cat(transitions.reward).unsqueeze(1).to(device)\n",
    "        \n",
    "        #clip reward between +1 and -1\n",
    "        reward_batch.data.clamp_(-1, 1)\n",
    "        \n",
    "        #next_state_batch = (V*C,H,W), where V is non_terminal next_state\n",
    "        non_terminal_next_state_batch = torch.cat([ns for ns in transitions.next_state if ns is not None]).to(device)\n",
    "        \n",
    "        #reshape to (N,C,H,W)\n",
    "        state_batch = state_batch.view(mem.batch_size, self.phi_length, self.env.size, self.env.size)\n",
    "        \n",
    "        #reshape to (V,C,H,W)\n",
    "        non_terminal_next_state_batch = non_terminal_next_state_batch.view(-1, self.phi_length, self.env.size, self.env.size)\n",
    "        \n",
    "        #get predicted Q values from model\n",
    "        Q_pred = self.model(state_batch)\n",
    "        \n",
    "        #get Q values of action taken, shape (N,1)\n",
    "        Q_vals = Q_pred.gather(1, action_batch)\n",
    "          \n",
    "        #get Q values from target model  \n",
    "        target_pred = self.target(non_terminal_next_state_batch)\n",
    "                        \n",
    "        #tensor for placing target values\n",
    "        target_vals = torch.zeros(mem.batch_size, 1).to(device) \n",
    "            \n",
    "        #fill in target values for non_terminal states\n",
    "        #the terminal states will stay initialized as zeros\n",
    "        target_vals[non_terminal_mask] = reward_batch[non_terminal_mask] + target_pred.max(1)[0].unsqueeze(1) * self.gamma\n",
    "            \n",
    "        #calculate loss between Q values and target values\n",
    "        loss = F.smooth_l1_loss(Q_vals, target_vals.detach())\n",
    "            \n",
    "        #zero gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        #calculate gradients \n",
    "        loss.backward()\n",
    "        \n",
    "        #clamp gradients\n",
    "        for p in self.model.parameters():\n",
    "            p.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        #update parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(GAME, PROCESSED_SIZE, SEED)\n",
    "mem = ReplayMemory(CAPACITY, BATCH_SIZE)\n",
    "model = DQN(N_ACTIONS)\n",
    "agent = Agent(env, mem, model, PHI_LENGTH, FRAME_SKIP, EPSILON_START, EPSILON_END, EPSILON_STEPS, GAMMA, TARGET_UPDATE, PRINT_UPDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 4, Steps: 5000, Epsilon: 1.00, Avg. Reward per Ep: -20.50\n",
      "Episodes: 8, Steps: 10000, Epsilon: 0.99, Avg. Reward per Ep: -20.00\n",
      "Episodes: 12, Steps: 15000, Epsilon: 0.99, Avg. Reward per Ep: -20.00\n",
      "Episodes: 15, Steps: 20000, Epsilon: 0.98, Avg. Reward per Ep: -20.00\n",
      "Episodes: 20, Steps: 25000, Epsilon: 0.98, Avg. Reward per Ep: -20.80\n",
      "Episodes: 24, Steps: 30000, Epsilon: 0.97, Avg. Reward per Ep: -20.50\n",
      "Episodes: 28, Steps: 35000, Epsilon: 0.97, Avg. Reward per Ep: -20.50\n",
      "Episodes: 32, Steps: 40000, Epsilon: 0.96, Avg. Reward per Ep: -20.75\n",
      "Episodes: 36, Steps: 45000, Epsilon: 0.96, Avg. Reward per Ep: -19.50\n"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
